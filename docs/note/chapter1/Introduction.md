# Introduction

我们定义输入为$\mathrm{x}$，输出为$y$。输入经常表示为向量$\mathrm{x}$因为一般有很多输入变量。目标$y$要么是连续的(在回归的例子中)，要么是离散的(在分类的例子中)。我们有包含$n$个观测的数据集$\mathcal{D}$，$\mathcal{D} = \{(\mathrm{x}_i,y_i)|i=1,\cdots,n\}$。

给定这个数据集我们想要对训练数据集中没有的新的输入$\mathrm{x}_*$做预测。我们需要从有限的训练数据$\mathcal{D}$转移到对所有可能的输入值进行预测的函数$f$。要做到这一点我们必须对函数的性质最初一些假设，否则任何与训练数据相符合的函数都有效。人们已经提出了各种各样的方法来处理监督学习问题，这里我们描述常见的两种方法。一种方法首先是限制我们考虑的函数的类别，例如，只考虑输入的线性函数。第二种方法是给每个可能的函数一个先验概率，其中我们认为更有可能的函数被赋予更高的概率，例如：因为它们比其他函数更平滑。第一种方法有一个显而易见的问题，我们需要决定我们所考虑函数类别的丰富度，如果我们在使用基于一个特定类别的函数的模型并且目标函数不能很好地用这类函数来建模，那么预测将会很差。有人可能会增强函数的灵活性，但是这样容易导致过拟合。

第二种方法也有一个严重的问题，因为可能的函数的数量是无限的，那么我们如何在有限的时间内去计算它们。这时高斯过程就来拯救我们了。高斯过程是高斯概率分布的推广。概率分布描述标量或向量型(对应于多元分布)的随机变量，而随机过程控制函数的属性。抛开数学复杂度不说，我们可以将函数看作是一个非常长的向量，向量里的每个元素在特定的输入$\mathrm{x}$上都满足函数值$f(x)$。事实证明，虽然这个想法有点天真，但它令人惊讶地接近了我们的需求。事实上，关于我们如何在计算上处理这些无穷维对象的问题已经有了令人愉悦的解决方案了：如果你只询问在有点的点处函数的性质，则如果你忽略无穷多的其他点，高斯过程中的推论同样会给你相同的答案，就像你把它们都考虑进去一样。高斯过程的主要吸引力之一恰恰在于它将复杂且一致的视图与计算处理能力统一起来。

